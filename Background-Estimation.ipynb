{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background estimation\n",
    "\n",
    "In this notebook we'll perform background estimation. We shall make use of the so called **ABCD method**. \n",
    "We describe the method briefly in what follows before applying it to our analysis.\n",
    "Let's assume we identify two variables ($v_1$ and $v_2$) which have uncorrelated distributions for the background.\n",
    "A cut each of these variables will have an efficiency which is indepedent of whatever selections have been performed on the other variable.\n",
    "Let's say we divide the 2D-plane ($v_1-v_2$) in four regions (A,B,C,D) being A is the final signal region where both selection requirements on $v_1$ and $v_2$ are applied. Such plane is represented in the following figure.\n",
    "\n",
    "![Representation of the 2D-plane used to define the ABCD regions for background estimation](img/abcd_method.png \"ABCD method\")\n",
    "\n",
    "If the selections are indeed uncorrelated for the background, then one can assume that the ratio of events in each regions can be related as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{N_{A}^{bkg}}{N_{B}^{bkg}} =\\frac{N_{C}^{bkg}}{N_{D}^{bkg}} \\Leftrightarrow N_{A}^{bkg} =N_{B}^{bkg}\\cdot\\frac{N_{C}^{bkg}}{N_{D}^{bkg}}\n",
    "\\end{equation}\n",
    "\n",
    "In the notebook below we shall apply this method to our analysis using two uncorrelated variables: the number of extra PF candidates and the $\\Delta\\xi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data and simulations to test the method\n",
    "\n",
    "The procedure used here is analogous to the one used in the Classification-Training notebook.\n",
    "After loading the data we run a prepare data method such that we can also run the classifier we have previously trained. In the prepare data method we'll already include the output of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snippets.GetData import *\n",
    "\n",
    "proton_selection = \"MultiRP\"\n",
    "PATH='/eos/user/c/cmsdas/long-exercises/pps-exclusive-dilepton/h5py'\n",
    "#PATH='output'\n",
    "#eras=['B','C','D','E','F'] #uncomment to use all data\n",
    "eras=['B']\n",
    "stream='El' # 'El' OR 'Mu'\n",
    "\n",
    "\n",
    "print('[Signal simulation]')\n",
    "df_signal,df_counts_signal = GetData(PATH+'/output-GGTo{}_Elastic_v0_signal_xa120_era2017_preTS2.h5'.format('EE' if stream=='El' else 'MuMu'))\n",
    "print('Selection counts')\n",
    "print(df_counts_signal)\n",
    "\n",
    "print('\\n')\n",
    "print('[Data (to be used as background)]')\n",
    "data_files = [PATH+'/output-UL2017{}-{}-Rand20.h5'.format(era,stream) for era in eras]\n",
    "df_bkg,df_counts_bkg = GetData(data_files,chunk_size=1000000)\n",
    "print('Selection counts')\n",
    "print(df_counts_bkg)\n",
    "print('\\n')\n",
    "\n",
    "print('[Data]')\n",
    "data_files = [PATH+'/output-UL2017{}-{}.h5'.format(era,stream) for era in eras]\n",
    "df_data,df_counts_data = GetData(data_files,chunk_size=1000000)\n",
    "print('Selection counts')\n",
    "print(df_counts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareData(df,clf=None,train_vars=['Lep0Pt', 'Lep1Pt', 'InvMass',   'XiMuMu', 'Xi', 'Acopl','ExtraPfCands_v1']):\n",
    "    \n",
    "    \"\"\"applies baseline selection cuts and adds the Xi resolution and the classifier prediction\"\"\"\n",
    "\n",
    "    msk = ( df[\"InvMass\"] >= 110. )\n",
    "\n",
    "    msk1 = None\n",
    "    msk2 = None\n",
    "    if proton_selection == \"SingleRP\":\n",
    "        # Single-RP in pixel stations\n",
    "        msk1_arm = ( df[\"RPId1\"] == 23 )\n",
    "        msk2_arm = ( df[\"RPId1\"] == 123 )\n",
    "        multiRP=0\n",
    "    elif proton_selection == \"MultiRP\":\n",
    "        # Multi-RP\n",
    "        msk1_arm = ( df[\"Arm\"] == 0 )\n",
    "        msk2_arm = ( df[\"Arm\"] == 1 )\n",
    "        multiRP=1\n",
    "   \n",
    "    #assign the xi from the dileptons based on the proton side\n",
    "    df[ \"XiMuMu\" ] = np.nan\n",
    "    df[ \"XiMuMu\" ].where( ~msk1_arm, df[ \"XiMuMuPlus\" ],  inplace=True )\n",
    "    df[ \"XiMuMu\" ].where( ~msk2_arm, df[ \"XiMuMuMinus\" ], inplace=True )\n",
    "\n",
    "    #add a resolution column\n",
    "    df[ 'DeltaXi'] = df[ \"XiMuMu\" ]-df[ \"Xi\" ]\n",
    "    df[ 'AbsDeltaXi'] = np.abs(df['DeltaXi'])\n",
    "    \n",
    "    #filter the data for reconstructed protons\n",
    "    msk1 = msk & ( df[\"MultiRP\"] == multiRP) & msk1_arm\n",
    "    msk2 = msk & ( df[\"MultiRP\"] == multiRP) & msk2_arm   \n",
    "    df=df[msk1 | msk2].copy()\n",
    "\n",
    "    #add the result of the classifier prediction\n",
    "    if clf and len(train_vars)>0:\n",
    "        X=df[train_vars]\n",
    "        df['clf_proba'] = clf.predict_proba(X)[:,1]\n",
    "        df['clf_categ'] = clf.predict(X).astype(int)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "#load the trained model\n",
    "from joblib import load\n",
    "clf = load('pps_longexercise_clf.joblib') \n",
    "\n",
    "df_signal_prep=PrepareData(df_signal,clf)\n",
    "df_bkg_prep=PrepareData(df_bkg,clf)\n",
    "df_data_prep=PrepareData(df_data,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a simple ABCD prediction\n",
    "\n",
    "In the following cell we plot the ABCD regions for signal, data and background in each one of them.\n",
    "For the data we select only the events which are categorized as background events (i.e. blind the final signal selection).\n",
    "What can you conclude from the following plots?\n",
    "\n",
    "**TASK 1**\n",
    "\n",
    "Compute the correlation of the two variables for each of the 3 datasets being plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "v1='AbsDeltaXi'\n",
    "cut_v1=0.01\n",
    "range_v1=(0,0.15)\n",
    "\n",
    "v2='ExtraPfCands_v1'\n",
    "cut_v2=6\n",
    "range_v2=(0,60)\n",
    "\n",
    "def doScatter(df,label, ax):\n",
    "    ax.hist2d(df[v1],df[v2],cmap='Blues')\n",
    "    ax.set_xlim(*range_v1)\n",
    "    ax.set_ylim(*range_v2)\n",
    "    ax.set_xlabel(v1)\n",
    "    ax.set_ylabel(v2)\n",
    "    ax.plot([cut_v1,cut_v1],[range_v2[0],range_v2[1]],color='lightgray')\n",
    "    ax.plot([range_v1[0],range_v1[1]],[cut_v2,cut_v2],color='lightgray')\n",
    "    ax.text(0.2,0.9,label, transform=ax.transAxes)\n",
    "    ax.text(cut_v1*0.7,cut_v2*0.7,'A')\n",
    "    ax.text(cut_v1*1.2,cut_v2*0.7,'B')\n",
    "    ax.text(cut_v1*0.7,cut_v2*1.2,'C')\n",
    "    ax.text(cut_v1*1.2,cut_v2*1.2,'D')\n",
    "    \n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "doScatter(df_signal_prep,'Signal',ax[0])\n",
    "doScatter(df_bkg_prep,'Background',ax[1])\n",
    "doScatter(df_data_prep[df_data_prep['clf_categ']==0],'Data (background category)',ax[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the ABCD method with a simple function... and the background is estimated!\n",
    "\n",
    "**TASK 2**\n",
    "\n",
    "Discusion: Is this method perfect? \n",
    "How is the closure affected if the cuts are different?\n",
    "Is there any extra dependency (e.g. on data-taking era, pileup, dilepton variables, etc.)?\n",
    "What are the final uncertainties we can associate to this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getABCDYields(df):\n",
    "    \n",
    "    \"\"\"gets the yields in the different abcd regions\"\"\"\n",
    "\n",
    "    mask_a=(df[v1]<cut_v1) &(df[v2]<cut_v2)\n",
    "    mask_b=(df[v1]>cut_v1) &(df[v2]<cut_v2)\n",
    "    mask_c=(df[v1]<cut_v1) &(df[v2]>cut_v2)\n",
    "    mask_d=(df[v1]>cut_v1) &(df[v2]>cut_v2)\n",
    "    \n",
    "    yields={'A':df[mask_a].shape[0],\n",
    "            'B':df[mask_b].shape[0],\n",
    "            'C':df[mask_c].shape[0],\n",
    "            'D':df[mask_d].shape[0]}\n",
    "\n",
    "    return yields\n",
    " \n",
    "def getABCDPrediction(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    starting from the yields in the different regions, applies the ABCD formula\n",
    "    returns the number of events observed in the signal region and the estimated number of events\n",
    "    \"\"\"\n",
    "    \n",
    "    yields=getABCDYields(df)\n",
    "    \n",
    "    return yields['A'],yields['B']*yields['C']/yields['D']\n",
    "    \n",
    "\n",
    "print('\\nData (background category)')\n",
    "nA,nAest=getABCDPrediction(df_data_prep[df_data_prep['clf_categ']==0])\n",
    "print('nA(obs)=%d nA(est)=%3.1f bias=%3.3f'%(nA,nAest,nAest/nA))\n",
    "\n",
    "print('\\nData (background)')\n",
    "nA,nAest=getABCDPrediction(df_bkg_prep)\n",
    "print('nA(obs)=%d nA(est)=%3.1f bias=%3.3f'%(nA,nAest,nAest/nA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gearing up for the statistical analysis\n",
    "\n",
    "In the following we'll prepare a so called datacard reporting the yields in each region for the signal and the background, as well as their associated uncertainties. This datacard will be used as input for the Higgs Combination tool. More details on this tool can be found in [here](https://cms-analysis.github.io/HiggsAnalysis-CombinedLimit).\n",
    "\n",
    "We start by preparing a ROOT file which will contain the counting histograms.\n",
    "For the moment this will be simple 1bin histograms with event counts but after you understand the example you can evolve this to be based on the distribution of a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "\n",
    "samples={'bkg':df_bkg_prep,\n",
    "         'sig':df_signal_prep,\n",
    "         'data_obs':df_data_prep}\n",
    "\n",
    "#loop over the samples\n",
    "#get yields in the different regions\n",
    "yields_abcd={}\n",
    "for df_name,df in samples.items():    \n",
    "    yields_abcd[df_name]=getABCDYields(df)\n",
    "    \n",
    "#apply a scale factor to the background estimation such that it matches the data in region D\n",
    "bkgSF=yields_abcd['data_obs']['D']/yields_abcd['bkg']['D']\n",
    "for x in yields_abcd['bkg']: \n",
    "    yields_abcd['bkg'][x]=bkgSF*yields_abcd['bkg'][x]\n",
    "print('Background estimation has been scaled by %3.3f'%bkgSF)\n",
    "\n",
    "#template for the shape of the distribution to analyze\n",
    "shapeH=ROOT.TH1F('shape','shape',1,0,1)\n",
    "\n",
    "#open the output file\n",
    "fOut=ROOT.TFile.Open('shapes.root','RECREATE')\n",
    "\n",
    "#loop over the samples\n",
    "for df_name in samples:    \n",
    "    \n",
    "    #save the counts in histogram and store them in the file\n",
    "    for region in yields:\n",
    "        shape=shapeH.Clone('{}_{}'.format(df_name,region))\n",
    "        shape.SetBinContent(1,yields_abcd[df_name][region])\n",
    "        shape.SetDirectory(fOut)\n",
    "        shape.Write()\n",
    "\n",
    "fOut.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a txt file to hold the datacard\n",
    "dc=open('datacard.dat','w')\n",
    "\n",
    "dc.write('imax *  number of channels\\n')\n",
    "dc.write('jmax *  number of processes -1\\n')\n",
    "dc.write('kmax *  number of nuisance parameters (sources of systematical uncertainties)\\n')\n",
    "dc.write('-------\\n')\n",
    "\n",
    "#spell out out the shapes are stored in the ROOT file\n",
    "dc.write('shapes * * shapes.root $PROCESS_$CHANNEL $PROCESS_$CHANNEL_$SYSTEMATIC\\n')\n",
    "\n",
    "#add the data observations\n",
    "dc.write('bin          A   B   C   D\\n')\n",
    "dc.write('observation  {0} {1} {2} {3}\\n'.format(yields_abcd['data_obs']['A'],\n",
    "                                                 yields_abcd['data_obs']['B'],\n",
    "                                                 yields_abcd['data_obs']['C'],\n",
    "                                                 yields_abcd['data_obs']['D']))\n",
    "dc.write('-------\\n')\n",
    "\n",
    "#add the expectations for signal and background\n",
    "dc.write('bin         A    A    B   B   C   C   D   D\\n')\n",
    "dc.write('process     sig  bkg  sig bkg sig bkg sig bkg\\n')\n",
    "dc.write('process     0    1    0   1   0   1   0   1\\n')\n",
    "dc.write('rate        {0:.3f}  {1:.3f}  {2:.3f} {3:.3f} {4:.3f} {5:.3f} {6:.3f} {7:.3f}\\n'.format(yields_abcd['sig']['A'],\n",
    "                                                                                                  yields_abcd['bkg']['A'],\n",
    "                                                                                                  yields_abcd['sig']['B'],\n",
    "                                                                                                  yields_abcd['bkg']['B'],\n",
    "                                                                                                  yields_abcd['sig']['C'],\n",
    "                                                                                                  yields_abcd['bkg']['C'],\n",
    "                                                                                                  yields_abcd['sig']['D'],\n",
    "                                                                                                  yields_abcd['bkg']['D'],\n",
    "                                                                                                 ))\n",
    "dc.write('-------\\n')\n",
    "\n",
    "#spell-out the ABCD method\n",
    "dc.write('alpha rateParam A bkg (@0*@1/@2) beta,gamma,delta\\n')\n",
    "dc.write('beta  rateParam B bkg {}\\n'.format(yields_abcd['bkg']['B']))\n",
    "dc.write('gamma rateParam C bkg {}\\n'.format(yields_abcd['bkg']['C']))\n",
    "dc.write('delta rateParam D bkg {}\\n'.format(yields_abcd['bkg']['D']))\n",
    "         \n",
    "#all done\n",
    "dc.close()\n",
    "\n",
    "print('datacard.dat is ready to be used with combine\\n\\n')\n",
    "!cat datacard.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Higgs Combination tool for the statistical analysis\n",
    "\n",
    "The short exercise on statistical analysis should be used as a reference - [link](https://cmsdas.github.io/statistics-short-exercise/).\n",
    "as well as the documentation of combine ([here](https://cms-analysis.github.io/HiggsAnalysis-CombinedLimit)). \n",
    "Some basic commands are listed below just to get started:\n",
    "    \n",
    "* Create the workspace: `text2workspace.py datacard.dat -o workspace.root`\n",
    "* Running fit diagnostics: `combine -M FitDiagnostics workspace.root -t -1 --expectSignal=1`\n",
    "* Compute the expected significance (asymptotic): `combine workspace.root -M Significance -t -1 --expectSignal=1`\n",
    "* Likelihood scan: `combine workspace.root -M MultiDimFit --algo grid --points 100 --fastScan --expectSignal=1 -t -1`\n",
    "* Compute the goodness-of-fit: `combine -M GoodnessOfFit workspace.root --algo=saturated --expectSignal=1 -t -1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
