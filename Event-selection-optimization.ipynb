{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization study notebook\n",
    "\n",
    "### Data exploration\n",
    "\n",
    "The first step of any analysis is to understand what we are searching for. In our analysis we aim to measure the central exclusive di-lepton production, $pp\\to p\\oplus \\ell\\ell \\oplus p$ process with $\\ell\\in\\{ e,\\mu \\} $. Feinman diagram of this process are shown bellow: \n",
    "\n",
    "<img src=\"img/diagrams.png\" alt=\"Feinmann diagrams\" style=\"width: 700px;\"/>\n",
    "\n",
    "Where in our case, we will consider only electrons and muons. \n",
    "\n",
    "---\n",
    "The measurement using 2016 data was published in [JHEP07(2018)153](https://arxiv.org/abs/1803.04496). This is the first time the 2017 data will be used to measure the central exclusive di-lepton production process at higher precision.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "<b>Remark</b>: Exclusive production of $\\tau$ leptons was not measured at the LHC yet since $\\tau$-leptons differ from electrons and muons by their relatively short lifetime ($c\\tau_0=87\\mu m$) and are observed only via their decay products. The main challenge with measuring $\\tau$s is elusive neutrinos (which escape detection). Hence measurement of the momentum of $\\tau$-lepton is tricky. Yet, it is possible because the opening angle between two daughter particles boosted with [lorentz factor](https://en.wikipedia.org/wiki/Lorentz_factor) $\\gamma$ is given by $\\theta ~\\sim 2/\\gamma$. With a large enough boost, the opening angle became collinear, and neutrino 4-momentum can be measured.\n",
    "\n",
    "To understand the process better, we will explore the final state signature. As we mentioned [earlier](https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCMSDataAnalysisSchoolCERN2020TaggedProtonsLongExercise#TASK_3_Optimization_study), samples are stored in the `h5py` data format, which can be easily accessed with Jupyter notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with standard python imports\n",
    "#!pip install --user mplhep #enable this line if the import of mplhep fails\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "from matplotlib.colors import LogNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make the plots in CMS style execute this line\n",
    "plt.style.use([hep.style.ROOT, hep.style.firamath])\n",
    "plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute this line if running on SWAN, otherwise update the path to the data files:\n",
    "PATH='/eos/user/c/cmsdas/long-exercises/pps-exclusive-dilepton/h5py'\n",
    "#PATH='output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (signal)\n",
    "\n",
    "We will load `h5py` files of the simulated signal events. Note that three different central exclusive di-lepton production processes are considered: exclusive, semi-exclusive, and inclusive (see Figure 1). We will load the files and convert them to pandas datafrme. Let's explore the differences between the processes.\n",
    "\n",
    "### Dataformat:\n",
    "\n",
    "We will use the following code `GetData(filename.h5)` to read the `h5` file and convert the data to pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(filename):\n",
    "    \n",
    "    \"\"\" opens a summary file and converts it to a pandas dataframe \"\"\"\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        dset = f['protons']\n",
    "        dset_columns = f['columns']\n",
    "        columns = list( dset_columns )\n",
    "        columns_str = [ item.decode(\"utf-8\") for item in columns ]\n",
    "        return pd.DataFrame( dset, columns=columns_str )\n",
    "    \n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the signal/background samples into the dataframes (takes some time)\n",
    "df_signal_excl = GetData(PATH+'/output-GGToEE_Elastic_v0_signal_xa120_era2017_preTS2.h5')\n",
    "df_background = GetData(PATH+'/output-MC13TeV_DYToLL50toInf_fxfx_v0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data samples into the dataframes (takes some time)\n",
    "df_data={}\n",
    "eras=['B']\n",
    "#eras=['B','C','D', E','F'] #uncooment to process all data\n",
    "for x in eras:\n",
    "    df_data[x] = GetData(PATH+'/output-UL2017{}-El.h5'.format(x))\n",
    "    df_data[x]['era']=x\n",
    "    print('output-UL2017{}-El shape = {}'.format(x,df_data[x].shape))\n",
    "\n",
    "#combine all into a single one\n",
    "df_data=pd.concat([df_data[x] for x in eras])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data files\n",
    "\n",
    "Similarly to what we did with ROOT files in the short exercise, let's look at the info we have in the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintInfoFromDF(df):\n",
    "    print('Print all branches:')\n",
    "    print(df.keys())\n",
    "    print('Size of the data is ',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PrintInfoFromDF(df_signal_excl)\n",
    "PrintInfoFromDF(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 38 different columns and 212744 raws in the file (each raw corresponds to a different event). In data we have added an extra column to flag which data-taking era is the event comming from.\n",
    "\n",
    "**TASK A**\n",
    "\n",
    "Look at distributions of different kinematic variables (among different processes) and try to see if you observe any difference... The code below `PlotFromDF(variable, dataframes, labels)` will plot normalized shapes of selected variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotFromDF(variable, xmin, xmax, nbins, dataframes, _labels, ax, log=False):\n",
    "    bins = np.linspace(xmin,xmax,nbins)\n",
    "    data=[]; labels=[]\n",
    "    for df, label in zip(dataframes, _labels):\n",
    "        h, _ = np.histogram(df[variable], bins,density=True)\n",
    "        data.append(h)\n",
    "        labels.append(label)\n",
    "    hep.histplot(data, bins, ax=ax, label=labels)\n",
    "    hep.cms.label(llabel=\"Preliminary\", rlabel=\"CMSvDAS 2020\", ax=ax)\n",
    "    ax.legend(); \n",
    "    ax.set(xlabel=variable, ylabel='p.d.f.')\n",
    "    if log: plt.yscale(\"log\")\n",
    "    plt.savefig(variable+'.png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will plot the di-lepton [acoplanarity](https://en.wikipedia.org/wiki/Acoplanarity) defined by:\n",
    "$$A = 1 - \\Delta\\phi(\\mu,\\mu)/\\pi$$\n",
    "\n",
    "In the exclusive events, due to absence of additional radiation, both leptons expected to be produced back-to-back, or with $\\Delta\\phi(\\mu,\\mu)\\sim\\pi$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be plotting MC prediction with the data, where the data is mostly populated with background events\n",
    "procc = [df_signal_excl,df_background,df_data]\n",
    "labels = ['Exclusive dilep','inclusive Z','data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "PlotFromDF('Acopl',0,1,100,procc,labels, ax, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next importants variables are the mass and the number of tracks associated to the primary vertex. In the ntuples we store two types of variables: \n",
    "- `ExtraPfCands` number of tracks measure from the PFlow candidates (for more info follow the [Tracks and Vertices](https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCMSDataAnalysisSchoolCERN2020TrackingAndVertexingShortExercise) short exercise)\n",
    "- `ExtraPfCands_v1` same as above but including only tracks with $|\\eta|<$2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "PlotFromDF('ExtraPfCands',0,20,20,procc,labels, ax, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "PlotFromDF('ExtraPfCands_v1',0,20,20,procc,labels, ax, log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "PlotFromDF('InvMass',0,800,100,procc,labels, ax, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of the signal region\n",
    "\n",
    "We see discrimination between our signal and the main background when plotting acoplanarity and track multiplicity variables.  It is common to define a figure of merit to choose the optimal selection cut. The simple approach is to ask: _Which cut will give us the highest_ $Z_0=\\frac{s}{\\sqrt{b}}$ _value_. Note, this figure of merit is valid for high background rate compared to the signal. However, a better approximation for the Poisson counting experiment, is to use $Z_0 = \\sqrt{2\\left( \\left(s+b\\right)\\ln(1+\\frac{s}{b})-s \\right)}$ (Eq. 97 in [Eur. Phys. J. C71, 1554 (2011)](https://arxiv.org/pdf/1007.1727.pdf)), but we can stick to $\\frac{s}{\\sqrt{b}}$.\n",
    "\n",
    "<b>TASK B</b>\n",
    "\n",
    "Write a code that computes significance for different selection cuts. Since we are interested in the relative estimate of the significance, compute the signal and background cut efficiencies:\n",
    "$$\\varepsilon_s/\\sqrt{\\varepsilon_b} = \\left(N_s^\\text{cut}/\\sqrt{N_b^\\text{cut}}\\right) / \\left(N_s^\\text{no-cut}/\\sqrt{N_b^\\text{no-cut}}\\right) = \\left(N_s^\\text{cut}/N_s^\\text{no-cut}\\right) / \\left(\\sqrt{N_b^\\text{cut} / N_b^\\text{no-cut}}\\right)$$\n",
    "\n",
    "as a funciton of the cut.\n",
    "\n",
    "Optimize selection for electrons and muons separately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that computes signal significance\n",
    "def computeZ(Ns, Nb):\n",
    "    return Ns/np.sqrt(Nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that computes event yields\n",
    "def ComputeYields(signal, background, variable, cut, cut_type):\n",
    "    if cut_type==-1:\n",
    "        Ns = signal[signal[variable]<cut].groupby('EventNum').ngroups\n",
    "        Nb = background[background[variable]<cut].groupby('EventNum').ngroups\n",
    "    elif cut_type==0:\n",
    "        Ns = signal[signal[variable]==cut].groupby('EventNum').ngroups\n",
    "        Nb = background[background[variable]==cut].groupby('EventNum').ngroups\n",
    "    elif cut_type==1:\n",
    "        Ns = signal[signal[variable]>cut].groupby('EventNum').ngroups\n",
    "        Nb = background[background[variable]>cut].groupby('EventNum').ngroups   \n",
    "    else: Ns=0; Nb=1; print('Error: wrong _sign_')\n",
    "    return Ns, Nb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Relative significance for a single cut value (example for Acopl<0.09):\n",
    "Nscut, Nbcut = ComputeYields(df_signal_excl,df_data,'Acopl',0.09,-1)\n",
    "Nsnocut, Nbnocut = ComputeYields(df_signal_excl,df_data,'Acopl',1,-1)\n",
    "sig_ratio = computeZ(Nscut, Nbcut) / computeZ(Nsnocut, Nbnocut)\n",
    "print('Relative significance = ',sig_ratio)\n",
    "print('Signal acceptance = ',Nscut/Nsnocut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
